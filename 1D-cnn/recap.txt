    #First CNN layer  with 32 filters, kernel 5
    cnn_model.add(Conv1D(filters=32, kernel_size=(5,), strides=1, padding='same',
                         activation=tf.keras.layers.LeakyReLU(alpha=0.001), input_shape = (x_train.shape[1:])))
    #Second CNN layer same as first
    cnn_model.add(Conv1D(filters=32, kernel_size=(5,), strides=1, padding='same',
                         activation=tf.keras.layers.LeakyReLU(alpha=0.001)))

    #Third CCN layer with 64 filter, kernel 3
    cnn_model.add(Conv1D(filters=64, kernel_size=(3,), strides=1, padding='same',
                         activation=tf.keras.layers.LeakyReLU(alpha=0.001)))
    #Fourth CNN layer same as third
    cnn_model.add(Conv1D(filters=64, kernel_size=(3,), strides=1,padding='same',
                         activation=tf.keras.layers.LeakyReLU(alpha=0.001)))
    
    #Fifth CNN layer with Max pooling
    cnn_model.add(MaxPool1D(pool_size=(3,), strides=2, padding='same'))
    cnn_model.add(Dropout(0.5))

    #Add a noise layer to make the model more robust and help predictions with noise
    cnn_model.add(GaussianNoise(0.1))
    
    #Flatten the output
    cnn_model.add(Flatten())
    
    #Add a dense layer with 64 neurons
    cnn_model.add(Dense(units = 64, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))
    #Add a dense layer with 128 neurons
    cnn_model.add(Dense(units = 128, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))
    #Add a dense layer with 128 neurons
    cnn_model.add(Dense(units = 128, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))






erste cnn, gegen 85/53 21s
lernt recht schnell, 8 epochs gegen 81%

######################################################################################
 #First CNN layer  with 32 filters, kernel 5
    cnn_model.add(Conv1D(filters=128, kernel_size=(50,), strides=3, padding='same',
                         activation=tf.keras.layers.LeakyReLU(alpha=0.001), input_shape = (x_train.shape[1:])))
    cnn_model.add(tf.keras.layers.BatchNormalization())
    cnn_model.add(MaxPool1D(pool_size=(2,), strides=3, padding='same'))
    #Second CNN layer same as first
    cnn_model.add(Conv1D(filters=32, kernel_size=(7,), strides=1, padding='same',
                         activation=tf.keras.layers.LeakyReLU(alpha=0.001)))
    cnn_model.add(tf.keras.layers.BatchNormalization())
    cnn_model.add(MaxPool1D(pool_size=(2,), strides=2, padding='same'))
    cnn_model.add(Dropout(0.5))
    #Third CCN layer with 64 filter, kernel 3
    cnn_model.add(Conv1D(filters=32, kernel_size=(9,), strides=1, padding='same',
                         activation=tf.keras.layers.LeakyReLU(alpha=0.001)))
    cnn_model.add(Dropout(0.5))
   

    #Add a noise layer to make the model more robust and help predictions with noise
    cnn_model.add(GaussianNoise(0.1))
    
    #Flatten the output
    cnn_model.add(Flatten())
    
   
    cnn_model.add(Dense(units = 128, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))
    
    #Softmax as last layer with 4 outputs
    cnn_model.add(Dense(units = 4, activation='softmax'))




zweite cnn, gegen 90/56  
		87/55
		87/54        11s
in 100epochs, lernt langsamer gegen 81% acc

60 epochs gegen 88/60 at 79-80%

link:
https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9294838

