{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac98d7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from predict import predict_labels\n",
    "from wettbewerb import load_references, save_predictions\n",
    "import time\n",
    "from score import score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pywt\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "from IPython.display import display\n",
    "from ecgdetectors import Detectors\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import filter_ecgToTest\n",
    "from filter_ecgToTest import filter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9cd1b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\t Dateien wurden geladen.\n"
     ]
    }
   ],
   "source": [
    "ecg_leads, ecg_labels, fs, ecg_names = load_references('training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37b94ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess the data and rename the labels into integers\n",
    "for idx, ecg_lead in enumerate(ecg_leads):\n",
    "    #ecg_leads[idx]=filter(ecg_leads[idx])\n",
    "    ecg_leads[idx]=ecg_lead/np.amax(ecg_leads[idx])\n",
    "    \n",
    "for idx, label in enumerate(ecg_labels):\n",
    "    if label==\"N\":\n",
    "        ecg_labels[idx]=0\n",
    "    if label==\"A\":\n",
    "        ecg_labels[idx]=1\n",
    "    if label==\"O\":\n",
    "        ecg_labels[idx]=2\n",
    "    if label==\"~\":\n",
    "        ecg_labels[idx]=3\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "55527640",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into training and validation datasets\n",
    "ecg_train, ecg_test, labels_train, labels_test,names_train,names_test = train_test_split(ecg_leads, \n",
    "                                                                  ecg_labels,ecg_names,\n",
    "                                                                  test_size=0.2, shuffle =True,stratify=ecg_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d4c68e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neurokit2 as nk\n",
    "import pandas as pd\n",
    "def split_HB(num_HB, ecg, labels,fs):\n",
    "    #this split the data in segments of 3 heart beats\n",
    "    hb=list()\n",
    "    hb_labels=list()\n",
    "    traductor=list()\n",
    "    numberIter=0\n",
    "    #this saves which split signal is part from which actual whole signal\n",
    "    decoder=list()\n",
    "    for idx, ecg_lead in enumerate(ecg):\n",
    "         #converts to a series\n",
    "        ecg_lead=pd.Series(ecg_lead)\n",
    "        #find the R peaks\n",
    "        try:\n",
    "            _, rpeaks = nk.ecg_peaks(ecg_lead,sampling_rate=fs)\n",
    "            rpeaks=rpeaks['ECG_R_Peaks']\n",
    "            prev_slice=rpeaks[0]\n",
    "            #this is the splitting of the signal\n",
    "            for count, posR in enumerate(rpeaks):\n",
    "                #only extract from the 3d peak\n",
    "                if (count%num_HB)==0 and len(ecg_lead[prev_slice:posR])>0:\n",
    "                    hb.append(ecg_lead[prev_slice:posR])\n",
    "                    hb_labels.append(labels[idx])\n",
    "                    decoder.append(idx)\n",
    "                    if len(ecg_lead[prev_slice:posR])>0:\n",
    "                        traductor.append((idx,numberIter))\n",
    "                        numberIter=numberIter+1\n",
    "                    prev_slice=posR\n",
    "        #not sure why we get errors, sad\n",
    "        except:\n",
    "            #we ignrore this signal xd, if that actually happends in the test data we solve later\n",
    "            e=0\n",
    "            print(\"ugh something went wrong\")\n",
    "    return hb, hb_labels,decoder\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ba6da3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE \n",
    "#numHB is an integer that gives the number of beats each signals will have from the original signal\n",
    "def generate_data_set(numHB,ecg,labels,name,freq,smote:str=\"no\"):\n",
    "    #first get the ecg split into hear beats\n",
    "    hb,hb_labels,decoder=split_HB(numHB,ecg,labels,freq)\n",
    "     #transforms it into a list of np array\n",
    "    temp=list()\n",
    "    for idx,ecg_lead in enumerate(hb): \n",
    "        temp.append(ecg_lead.to_numpy())\n",
    "    \n",
    "    #make all heart beat signals the same length, for larger signal cut them, for shorter signals just add 0s until it matches the desired length\n",
    "    minL=1000\n",
    "    #new approach to delete the other signals that are too small\n",
    "    new_ecg=list()\n",
    "    new_labels=list()\n",
    "    #instead of removing them, we want to do something different, add them in another list\n",
    "    for idx, ecg_lead in enumerate(temp):\n",
    "        if len(ecg_lead)>=minL:\n",
    "            new_ecg.append(ecg_lead[:minL])\n",
    "            new_labels.append(hb_labels[idx])\n",
    "            #new_ecg_names.append(ecg_names[idx])\n",
    "        else:\n",
    "            toAdd=minL-len(ecg_lead)\n",
    "            toAdd=np.zeros(toAdd)\n",
    "            #merges the signal that was too short with signal full of 0 to achive the desired lenght\n",
    "            new_ecg.append(np.concatenate((ecg_lead, toAdd)))\n",
    "            new_labels.append(hb_labels[idx])\n",
    "            #new_ecg_names.append(ecg_names[idx])\n",
    "    if smote==\"smote\":\n",
    "        sm = SMOTE(random_state=42)\n",
    "        new_ecg, new_labels = sm.fit_resample(new_ecg, new_labels)\n",
    "\n",
    "    #now convert the data into working dataset for the cnn model\n",
    "    \n",
    " \n",
    "    x=new_ecg\n",
    "    x=np.array(x)\n",
    "    x=x.reshape((x.shape[0],x.shape[1],1))\n",
    "    \n",
    "    y=np.array(new_labels)\n",
    "    return x,y,decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "798f8808",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir=\"cnn_1D\"\n",
    "model_dir=os.path.join(os.getcwd(),model_dir)\n",
    "pred_dir=model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "126ec7f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "ugh something went wrong\n",
      "ugh something went wrong\n",
      "ugh something went wrong\n"
     ]
    }
   ],
   "source": [
    "x_test,y_test,decoder_test=generate_data_set(3,ecg_test,labels_test,names_test,fs)\n",
    "print(\"----------\")\n",
    "x_train,y_train,decoder_train=generate_data_set(3,ecg_train,labels_train,names_train,fs,\"smote\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a12c30da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#CNN\n",
    "from tensorflow.keras.layers import Flatten, Dense, Conv1D, MaxPool1D, Dropout\n",
    "# Create sequential model \n",
    "cnn_model = tf.keras.models.Sequential()\n",
    "#First CNN layer  with 32 filters, conv window 3, relu activation and same padding\n",
    "cnn_model.add(Conv1D(filters=32, kernel_size=(3,), strides=1, padding='same', activation=tf.keras.layers.LeakyReLU(alpha=0.001), input_shape = (x_train.shape[1:])))\n",
    "#Second CNN layer  with 64 filters, conv window 3, relu activation and same padding\n",
    "cnn_model.add(Conv1D(filters=64, kernel_size=(3,), strides=1, padding='same', activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n",
    "#Third CNN layer with 128 filters, conv window 3, relu activation and same padding\n",
    "cnn_model.add(Conv1D(filters=128, kernel_size=(3,), strides=1,padding='same', activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n",
    "#Fourth CNN layer with Max pooling\n",
    "cnn_model.add(MaxPool1D(pool_size=(3,), strides=2, padding='same'))\n",
    "cnn_model.add(Dropout(0.5))\n",
    "#Flatten the output\n",
    "cnn_model.add(Flatten())\n",
    "#Add a dense layer with 256 neurons\n",
    "cnn_model.add(Dense(units = 64, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n",
    "#Add a dense layer with 512 neurons\n",
    "cnn_model.add(Dense(units = 128, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n",
    "#Add a dense layer with 512 neurons\n",
    "cnn_model.add(Dense(units = 256, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n",
    "#Softmax as last layer with five outputs\n",
    "cnn_model.add(Dense(units = 4, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dacecd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_9 (Conv1D)            (None, 1000, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 1000, 64)          6208      \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 1000, 128)         24704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 64)                4096064   \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 4)                 1028      \n",
      "=================================================================\n",
      "Total params: 4,169,476\n",
      "Trainable params: 4,169,476\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_model.compile(optimizer='adam', loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6b80528c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "8596/8596 [==============================] - 41s 5ms/step - loss: 1.4546 - accuracy: 0.5580 - val_loss: 1.3442 - val_accuracy: 0.4888\n",
      "Epoch 2/3\n",
      "8596/8596 [==============================] - 42s 5ms/step - loss: 0.8915 - accuracy: 0.7230 - val_loss: 1.2486 - val_accuracy: 0.5443\n",
      "Epoch 3/3\n",
      "8596/8596 [==============================] - 41s 5ms/step - loss: 0.7265 - accuracy: 0.7666 - val_loss: 1.1991 - val_accuracy: 0.5622\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "batch_size = 16\n",
    "weights = {0: 1.,1: 10.,2: 1.,3: 1}\n",
    "from keras import callbacks\n",
    "earlystopping = callbacks.EarlyStopping(monitor =\"val_loss\", mode =\"min\", patience = 5, restore_best_weights = True)\n",
    "history = cnn_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data = (x_test, y_test),\n",
    "class_weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b516c9b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200\t Labels wurden geschrieben.\n",
      "1200\t Labels wurden geschrieben.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7586206896551724,\n",
       " 0.5090805470476835,\n",
       " {'N': {'n': 557, 'a': 40, 'o': 92, 'p': 27},\n",
       "  'A': {'n': 9, 'a': 88, 'o': 5, 'p': 2},\n",
       "  'O': {'n': 147, 'a': 99, 'o': 91, 'p': 6},\n",
       "  'P': {'n': 10, 'a': 7, 'o': 2, 'p': 18}})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statistics\n",
    "from statistics import mode\n",
    "#get the predictions\n",
    "classes = cnn_model.predict(x_test)\n",
    "predictions=list()\n",
    "for idx, ecg_lead in enumerate (classes):\n",
    "    predictions.append(np.argmax(ecg_lead))\n",
    "\n",
    "#this dictionary will save all the predictions in groups\n",
    "label_decoder={}\n",
    "for idx, label in enumerate(predictions):\n",
    "    if decoder_test[idx] in label_decoder:\n",
    "        label_decoder[decoder_test[idx]].append(label)\n",
    "    else:\n",
    "        label_decoder[decoder_test[idx]]=list()\n",
    "        label_decoder[decoder_test[idx]].append(label)\n",
    "#to get the most frequent prediction for each signal \n",
    "labels_toTest=list()\n",
    "labels_toSave=list()\n",
    "for idx,keys in enumerate(label_decoder):\n",
    "    #get the most frequent prediction\n",
    "    frequent_label=mode(label_decoder[keys])\n",
    "    #convert it into a letter\n",
    "    labels2give=\"N\"\n",
    "    if int(frequent_label)==0:\n",
    "        labels2give=\"N\"\n",
    "    elif int(frequent_label)==1:\n",
    "        labels2give=\"A\"\n",
    "    elif int(frequent_label)==2:\n",
    "        labels2give=\"O\"\n",
    "    else:\n",
    "        labels2give=\"~\"\n",
    "    #save the final predictred labels\n",
    "    labels_toTest.append((keys,labels2give))\n",
    "\n",
    "for i,ecg in enumerate(labels_test):\n",
    "    ecg_class=int(ecg)\n",
    "    label2give=\"N\"\n",
    "    if int(ecg_class)==0:\n",
    "        labels2give=\"N\"\n",
    "    elif int(ecg_class)==1:\n",
    "        labels2give=\"A\"\n",
    "    elif int(ecg_class)==2:\n",
    "        labels2give=\"O\"\n",
    "    else:\n",
    "        labels2give=\"~\"\n",
    "    #esto es para el name\n",
    "    labels_toSave.append((i,labels2give))\n",
    "\n",
    "save_predictions(labels_toTest,model_dir,\"PREDICTIONS\")\n",
    "save_predictions(labels_toSave,model_dir,\"REFERENCE\")\n",
    "score(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "33f88aa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 2, 3, 0, 0, 1, 0, 2, 1, 3, 0, 0]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_decoder[keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de99125a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1198"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
