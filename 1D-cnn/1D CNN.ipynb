{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac98d7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from predict import predict_labels\n",
    "from wettbewerb import load_references, save_predictions\n",
    "import time\n",
    "from score import score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pywt\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "from IPython.display import display\n",
    "from ecgdetectors import Detectors\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import filter_ecgToTest\n",
    "from filter_ecgToTest import filter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d9cd1b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\t Dateien wurden geladen.\n"
     ]
    }
   ],
   "source": [
    "ecg_leads, ecg_labels, fs, ecg_names = load_references('training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "37b94ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess the data and rename the labels into integers\n",
    "for idx, ecg_lead in enumerate(ecg_leads):\n",
    "    ecg_leads[idx]=filter(ecg_leads[idx])\n",
    "    ecg_leads[idx]=ecg_lead/np.amax(ecg_leads[idx])\n",
    "    \n",
    "for idx, label in enumerate(ecg_labels):\n",
    "    if label==\"N\":\n",
    "        ecg_labels[idx]=0\n",
    "    if label==\"A\":\n",
    "        ecg_labels[idx]=1\n",
    "    if label==\"O\":\n",
    "        ecg_labels[idx]=2\n",
    "    if label==\"~\":\n",
    "        ecg_labels[idx]=3\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "55527640",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into training and validation datasets\n",
    "ecg_train, ecg_test, labels_train, labels_test,names_train,names_test = train_test_split(ecg_leads, \n",
    "                                                                  ecg_labels,ecg_names,\n",
    "                                                                  test_size=0.2, shuffle =True,stratify=ecg_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "5fd1ec14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finds the optimal value for the signal segmentation length from the traing dataset\n",
    "import statistics\n",
    "def find_optimal_length(hb):\n",
    "    opt_len=list()\n",
    "    for idx, ecg in enumerate(hb):\n",
    "        opt_len.append(len(ecg))\n",
    "    opt_len=statistics.median(opt_len)\n",
    "    #opt_len=mode(opt_len)\n",
    "    return opt_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d4c68e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neurokit2 as nk\n",
    "import pandas as pd\n",
    "from ecgdetectors import Detectors\n",
    "def split_HB(num_HB, ecg, labels,names,fs):\n",
    "    \n",
    "    #this split the data in segments of 3 heart beats\n",
    "    hb=list()\n",
    "    hb_labels=list()\n",
    "    #this saves which split signal is part from which actual whole signal\n",
    "    decoder=list()\n",
    "    #start detectors as alternative\n",
    "    detectors=Detectors(fs)\n",
    "    for idx, ecg_lead in enumerate(ecg):\n",
    "         #converts to a series\n",
    "        ecg_lead=pd.Series(ecg_lead)\n",
    "        #find the R peaks\n",
    "        try:\n",
    "            _, rpeaks = nk.ecg_peaks(ecg_lead,sampling_rate=fs)\n",
    "            rpeaks=rpeaks['ECG_R_Peaks']\n",
    "            prev_slice=rpeaks[0]\n",
    "\n",
    "        #if we cant find the r peaks because sadge, then use other detector\n",
    "        except:\n",
    "            rpeaks = detectors.pan_tompkins_detector(ecg_lead)\n",
    "            prev_slice=rpeaks[0]\n",
    "            print(\"┬┬﹏┬┬\")\n",
    "            #this is the splitting of the signal\n",
    "        for count, posR in enumerate(rpeaks):\n",
    "            #only extract from the 3d peak\n",
    "            if (count%num_HB)==0 and len(ecg_lead[prev_slice:posR])>0:\n",
    "                hb.append(ecg_lead[prev_slice:posR])\n",
    "                hb_labels.append(labels[idx])\n",
    "                decoder.append(names[idx])\n",
    "                prev_slice=posR\n",
    "    return hb, hb_labels,decoder\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "0ba6da3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE \n",
    "from sklearn.decomposition import PCA\n",
    "#numHB is an integer that gives the number of beats each signals will have from the original signal\n",
    "def generate_data_set(numHB,ecg,labels,names,freq,smote:bool=False,minL:int=0,pca_prev=None,pca:bool=False):\n",
    "    #data will be returen as a dictionary with the desired variables \n",
    "    data={}\n",
    "    #first get the ecg split into hear beats\n",
    "    hb,hb_labels,decoder=split_HB(numHB,ecg,labels,names,freq)\n",
    "    #find the optimal length if minL was not given\n",
    "    if minL<=0:\n",
    "        minL=int(find_optimal_length(hb))\n",
    "        \n",
    "     #transforms it into a list of np array\n",
    "    temp=list()\n",
    "    for idx,ecg_lead in enumerate(hb): \n",
    "        temp.append(ecg_lead.to_numpy())\n",
    "    \n",
    "    #make all heart beat signals the same length, for larger signal cut them, for shorter signals just add 0s until it matches the desired length\n",
    "  \n",
    "    #new approach to delete the other signals that are too small\n",
    "    new_ecg=list()\n",
    "    new_labels=list()\n",
    "    #instead of removing them, we want to do something different, add them in another list\n",
    "    for idx, ecg_lead in enumerate(temp):\n",
    "        if len(ecg_lead)>=minL:\n",
    "            new_ecg.append(ecg_lead[:minL])\n",
    "            new_labels.append(hb_labels[idx])\n",
    "            #new_ecg_names.append(ecg_names[idx])\n",
    "        else:\n",
    "            toAdd=minL-len(ecg_lead)\n",
    "            toAdd=np.zeros(toAdd)\n",
    "            #merges the signal that was too short with signal full of 0 to achive the desired lenght\n",
    "            new_ecg.append(np.concatenate((ecg_lead, toAdd)))\n",
    "            new_labels.append(hb_labels[idx])\n",
    "            #new_ecg_names.append(ecg_names[idx])\n",
    "    if smote:\n",
    "        sm = SMOTE()\n",
    "        new_ecg, new_labels = sm.fit_resample(new_ecg, new_labels)\n",
    "\n",
    "    \n",
    "    #only use pca if we say so\n",
    "    if pca:\n",
    "        #if we dont send any older pca, then create its own\n",
    "        if pca_prev is None:\n",
    "            pca_prev=PCA(.99)\n",
    "            pca_prev.fit(new_ecg)\n",
    "        #use the pca to reduce the dimension of the data\n",
    "        new_ecg=pca_prev.transform(new_ecg)\n",
    "        data['pca']=pca_prev\n",
    "\n",
    " #now convert the data into working dataset for the cnn model\n",
    "    x=new_ecg\n",
    "    x=np.array(x)\n",
    "    x=x.reshape((x.shape[0],x.shape[1],1))\n",
    "    \n",
    "    y=np.array(new_labels)\n",
    "    \n",
    "    #return the data in a dictionary way, to simplily later use of the desired variables\n",
    "   \n",
    "    data['input']=x\n",
    "    data['labels']=y\n",
    "    data['decoder']=decoder\n",
    "    data['optimal_length']=minL\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "798f8808",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir=\"cnn_1D\"\n",
    "model_dir=os.path.join(os.getcwd(),model_dir)\n",
    "pred_dir=model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7fb092",
   "metadata": {},
   "source": [
    "### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "126ec7f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┬┬﹏┬┬\n",
      "┬┬﹏┬┬\n",
      "┬┬﹏┬┬\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "train=generate_data_set(3,ecg_train,labels_train,names_train,fs,\n",
    "                        smote=True,\n",
    "                        pca=False\n",
    "                       )\n",
    "#train is a dictionary, get the actual variables\n",
    "x_train=train['input']\n",
    "y_train=train['labels']\n",
    "opt_length=train['optimal_length']\n",
    "try:\n",
    "    pca_train=train['pca']\n",
    "except:\n",
    "    pca_train=None\n",
    "print(\"----------\")\n",
    "test=generate_data_set(3,ecg_test,labels_test,names_test,fs,minL=opt_length\n",
    "                      ,pca_prev=pca_train\n",
    "                       ,pca=False\n",
    "                      )\n",
    "x_test=test['input']\n",
    "y_test=test['labels']\n",
    "decoder_test=test['decoder']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "a12c30da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#CNN\n",
    "from tensorflow.keras.layers import Flatten, Dense, Conv1D, MaxPool1D, Dropout\n",
    "# Create sequential model \n",
    "cnn_model = tf.keras.models.Sequential()\n",
    "#First CNN layer  with 32 filters, conv window 3, relu activation and same padding\n",
    "cnn_model.add(Conv1D(filters=32, kernel_size=(3,), strides=1, padding='same', activation=tf.keras.layers.LeakyReLU(alpha=0.001), input_shape = (x_train.shape[1:])))\n",
    "#Second CNN layer  with 64 filters, conv window 3, relu activation and same padding\n",
    "cnn_model.add(Conv1D(filters=64, kernel_size=(3,), strides=1, padding='same', activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n",
    "#Third CNN layer with 128 filters, conv window 3, relu activation and same padding\n",
    "cnn_model.add(Conv1D(filters=128, kernel_size=(3,), strides=1,padding='same', activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n",
    "#Fourth CNN layer with Max pooling\n",
    "cnn_model.add(MaxPool1D(pool_size=(3,), strides=2, padding='same'))\n",
    "cnn_model.add(Dropout(0.5))\n",
    "#Flatten the output\n",
    "cnn_model.add(Flatten())\n",
    "#Add a dense layer with 256 neurons\n",
    "cnn_model.add(Dense(units = 64, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n",
    "#Add a dense layer with 512 neurons\n",
    "cnn_model.add(Dense(units = 128, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n",
    "#Add a dense layer with 512 neurons\n",
    "cnn_model.add(Dense(units = 256, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n",
    "#Softmax as last layer with five outputs\n",
    "cnn_model.add(Dense(units = 4, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "dacecd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_99 (Conv1D)           (None, 715, 32)           128       \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 715, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 715, 128)          24704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 358, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 358, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_33 (Flatten)         (None, 45824)             0         \n",
      "_________________________________________________________________\n",
      "dense_132 (Dense)            (None, 64)                2932800   \n",
      "_________________________________________________________________\n",
      "dense_133 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_134 (Dense)            (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_135 (Dense)            (None, 4)                 1028      \n",
      "=================================================================\n",
      "Total params: 3,006,212\n",
      "Trainable params: 3,006,212\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "opt=tf.keras.optimizers.RMSprop(learning_rate=0.0001)\n",
    "cnn_model.compile(optimizer='adam', loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "6b80528c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "4299/4299 [==============================] - 24s 5ms/step - loss: 1.4150 - accuracy: 0.5744 - val_loss: 1.3992 - val_accuracy: 0.4761\n",
      "Epoch 2/3\n",
      "4299/4299 [==============================] - 24s 5ms/step - loss: 0.8511 - accuracy: 0.7305 - val_loss: 1.2191 - val_accuracy: 0.5578\n",
      "Epoch 3/3\n",
      "4299/4299 [==============================] - 24s 6ms/step - loss: 0.6755 - accuracy: 0.7762 - val_loss: 1.1657 - val_accuracy: 0.5901\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "batch_size = 32\n",
    "weights = {0: 1.,1: 10.,2: 1.,3: 1}\n",
    "from keras import callbacks\n",
    "earlystopping = callbacks.EarlyStopping(monitor =\"val_loss\", mode =\"min\", patience = 5, restore_best_weights = True)\n",
    "history = cnn_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data = (x_test, y_test),\n",
    "class_weight=weights,\n",
    "#callbacks =[earlystopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "b516c9b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200\t Labels wurden geschrieben.\n",
      "1200\t Labels wurden geschrieben.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8495575221238938,\n",
       " 0.5014962786669622,\n",
       " {'N': {'n': 625, 'a': 26, 'o': 51, 'p': 14},\n",
       "  'A': {'n': 6, 'a': 96, 'o': 1, 'p': 1},\n",
       "  'O': {'n': 154, 'a': 109, 'o': 70, 'p': 10},\n",
       "  'P': {'n': 10, 'a': 10, 'o': 5, 'p': 12}})"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statistics\n",
    "from statistics import mode\n",
    "#get the predictions\n",
    "classes = cnn_model.predict(x_test)\n",
    "predictions=list()\n",
    "for idx, ecg_lead in enumerate (classes):\n",
    "    predictions.append(np.argmax(ecg_lead))\n",
    "\n",
    "#this dictionary will save all the predictions in groups\n",
    "label_decoder={}\n",
    "for idx, label in enumerate(predictions):\n",
    "    if decoder_test[idx] in label_decoder:\n",
    "        label_decoder[decoder_test[idx]].append(label)\n",
    "    else:\n",
    "        label_decoder[decoder_test[idx]]=list()\n",
    "        label_decoder[decoder_test[idx]].append(label)\n",
    "#to get the most frequent prediction for each signal \n",
    "labels_toTest=list()\n",
    "labels_toSave=list()\n",
    "for idx,keys in enumerate(label_decoder):\n",
    "    #get the most frequent prediction\n",
    "    frequent_label=mode(label_decoder[keys])\n",
    "    #convert it into a letter\n",
    "    labels2give=\"N\"\n",
    "    if int(frequent_label)==0:\n",
    "        labels2give=\"N\"\n",
    "    elif int(frequent_label)==1:\n",
    "        labels2give=\"A\"\n",
    "    elif int(frequent_label)==2:\n",
    "        labels2give=\"O\"\n",
    "    else:\n",
    "        labels2give=\"~\"\n",
    "    #save the final predictred labels\n",
    "    labels_toTest.append((keys,labels2give))\n",
    "\n",
    "for i,ecg in enumerate(labels_test):\n",
    "    ecg_class=int(ecg)\n",
    "    label2give=\"N\"\n",
    "    if int(ecg_class)==0:\n",
    "        labels2give=\"N\"\n",
    "    elif int(ecg_class)==1:\n",
    "        labels2give=\"A\"\n",
    "    elif int(ecg_class)==2:\n",
    "        labels2give=\"O\"\n",
    "    else:\n",
    "        labels2give=\"~\"\n",
    "    #esto es para el name\n",
    "    labels_toSave.append((names_test[i],labels2give))\n",
    "\n",
    "save_predictions(labels_toTest,model_dir,\"PREDICTIONS\")\n",
    "save_predictions(labels_toSave,model_dir,\"REFERENCE\")\n",
    "score(model_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
