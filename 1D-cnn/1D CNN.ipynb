{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "673f9853",
   "metadata": {},
   "source": [
    "## Example code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0301b668",
   "metadata": {},
   "source": [
    "### Imports for the usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58d504fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wettbewerb import load_references, save_predictions\n",
    "from score import score\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from train_CNN import train\n",
    "from predict_CNN import predict_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac5a7ba",
   "metadata": {},
   "source": [
    "### Get signals and split the datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9cd1b0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\t Dateien wurden geladen.\n"
     ]
    }
   ],
   "source": [
    "ecg_leads, ecg_labels, fs, ecg_names = load_references('training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b7a1f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_train, ecg_test, labels_train, labels_test,names_train,names_test = train_test_split(ecg_leads, \n",
    "                                                                  ecg_labels,ecg_names,\n",
    "                                                                  test_size=0.2, shuffle =True,stratify=ecg_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9414e1",
   "metadata": {},
   "source": [
    "### Train and save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2457313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "2153/2153 [==============================] - 16s 7ms/step - loss: 1.9104 - accuracy: 0.5392\n",
      "Epoch 2/8\n",
      "2153/2153 [==============================] - 15s 7ms/step - loss: 1.3047 - accuracy: 0.6783\n",
      "Epoch 3/8\n",
      "2153/2153 [==============================] - 15s 7ms/step - loss: 1.0634 - accuracy: 0.7248\n",
      "Epoch 4/8\n",
      "2153/2153 [==============================] - 15s 7ms/step - loss: 0.9268 - accuracy: 0.7516\n",
      "Epoch 5/8\n",
      "2153/2153 [==============================] - 15s 7ms/step - loss: 0.8179 - accuracy: 0.7748\n",
      "Epoch 6/8\n",
      "2153/2153 [==============================] - 15s 7ms/step - loss: 0.7462 - accuracy: 0.7912\n",
      "Epoch 7/8\n",
      "2153/2153 [==============================] - 15s 7ms/step - loss: 0.6960 - accuracy: 0.8028\n",
      "Epoch 8/8\n",
      "2153/2153 [==============================] - 15s 7ms/step - loss: 0.6455 - accuracy: 0.8139\n"
     ]
    }
   ],
   "source": [
    "train(ecg_train,labels_train,names_train,fs,3,model_name='Abgabe3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7c8c5a",
   "metadata": {},
   "source": [
    "### Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88d8eb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_predicted=predict_labels(ecg_leads=ecg_test,fs=fs,ecg_names=names_test,model_name='Abgabe3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fb5811",
   "metadata": {},
   "source": [
    "### Calculate score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "15fc4e68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200\t Labels wurden geschrieben.\n",
      "1200\t Labels wurden geschrieben.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8191489361702128,\n",
       " 0.6047021997974172,\n",
       " {'N': {'n': 674, 'a': 7, 'o': 27, 'p': 8},\n",
       "  'A': {'n': 14, 'a': 77, 'o': 9, 'p': 4},\n",
       "  'O': {'n': 200, 'a': 38, 'o': 102, 'p': 3},\n",
       "  'P': {'n': 11, 'a': 3, 'o': 6, 'p': 17}})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dir=\"cnn_1D\"\n",
    "pred_dir=os.path.join(os.getcwd(),pred_dir)\n",
    "labels_true=list()\n",
    "for i,label in enumerate(labels_test):\n",
    "    labels_true.append((names_test[i],label))\n",
    "\n",
    "save_predictions(labels_predicted,pred_dir,\"PREDICTIONS\")\n",
    "save_predictions(labels_true,pred_dir,\"REFERENCE\")\n",
    "score(pred_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbb4c67",
   "metadata": {},
   "source": [
    "## Full code\n",
    "#### (some parts may be outdated, check train_CNN.py, predict_CNN.py and process_data.py to find the up to date functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2d7c9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wettbewerb import load_references, save_predictions\n",
    "from score import score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from ecgdetectors import Detectors\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import statistics\n",
    "from statistics import mode\n",
    "from process_data import filter_ecg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9b88bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\t Dateien wurden geladen.\n"
     ]
    }
   ],
   "source": [
    "ecg_leads, ecg_labels, fs, ecg_names = load_references('training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37b94ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess the data and rename the labels into integers\n",
    "\n",
    "for idx, ecg_lead in enumerate(ecg_leads):\n",
    "    ecg_leads[idx]=filter_ecg(ecg_leads[idx])\n",
    "    ecg_leads[idx]=ecg_lead/np.amax(ecg_leads[idx])\n",
    "    \n",
    "for idx, label in enumerate(ecg_labels):\n",
    "    if label==\"N\":\n",
    "        ecg_labels[idx]=0\n",
    "    if label==\"A\":\n",
    "        ecg_labels[idx]=1\n",
    "    if label==\"O\":\n",
    "        ecg_labels[idx]=2\n",
    "    if label==\"~\":\n",
    "        ecg_labels[idx]=3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55527640",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into training and validation datasets\n",
    "ecg_train, ecg_test, labels_train, labels_test,names_train,names_test = train_test_split(ecg_leads, \n",
    "                                                                  ecg_labels,ecg_names,\n",
    "                                                                  test_size=0.2, shuffle =True,stratify=ecg_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a7a5dd",
   "metadata": {},
   "source": [
    "### Usefull functions \n",
    "#### (some may outdated, check the process_data.py to find the up to date functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65082427",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finds the optimal value for the signal segmentation length from the traing dataset\n",
    "import statistics\n",
    "def find_optimal_length(hb):\n",
    "    opt_len=list()\n",
    "    for idx, ecg in enumerate(hb):\n",
    "        opt_len.append(len(ecg))\n",
    "    opt_len=statistics.median(opt_len)\n",
    "    #opt_len=mode(opt_len)\n",
    "    return opt_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4c68e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#segment the data \n",
    "import neurokit2 as nk\n",
    "import pandas as pd\n",
    "from ecgdetectors import Detectors\n",
    "def split_HB(num_HB, ecg, labels,names,fs):\n",
    "    \n",
    "    #this split the data in segments of 3 heart beats\n",
    "    hb=list()\n",
    "    hb_labels=list()\n",
    "    #this saves which split signal is part from which actual whole signal\n",
    "    decoder=list()\n",
    "    #start detectors as alternative\n",
    "    detectors=Detectors(fs)\n",
    "    for idx, ecg_lead in enumerate(ecg):\n",
    "         #converts to a series\n",
    "        ecg_lead=pd.Series(ecg_lead)\n",
    "        #find the R peaks\n",
    "        try:\n",
    "            _, rpeaks = nk.ecg_peaks(ecg_lead,sampling_rate=fs)\n",
    "            rpeaks=rpeaks['ECG_R_Peaks']\n",
    "            prev_slice=rpeaks[0]\n",
    "\n",
    "        #if we cant find the r peaks because sadge, then use other detector\n",
    "        except:\n",
    "            rpeaks = detectors.pan_tompkins_detector(ecg_lead)\n",
    "            prev_slice=rpeaks[0]\n",
    "            #this is the splitting of the signal\n",
    "        if len(rpeaks)<2*num_HB:\n",
    "            rpeaks = detectors.pan_tompkins_detector(ecg_lead)\n",
    "            prev_slice=rpeaks[0]\n",
    "        for count, posR in enumerate(rpeaks):\n",
    "            #only extract from the 3d peak\n",
    "            if (count%num_HB)==0 and len(ecg_lead[prev_slice:posR])>0:\n",
    "                hb.append(ecg_lead[prev_slice:posR])\n",
    "                hb_labels.append(labels[idx])\n",
    "                decoder.append(names[idx])\n",
    "                prev_slice=posR\n",
    "    return hb, hb_labels,decoder\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ba6da3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE \n",
    "from sklearn.decomposition import PCA\n",
    "#numHB is an integer that gives the number of beats each signals will have from the original signal\n",
    "def generate_data_set(numHB,ecg,labels,names,freq,smote:bool=False,minL:int=0,pca_prev=None,pca:bool=False):\n",
    "    #data will be returen as a dictionary with the desired variables \n",
    "    data={}\n",
    "    #first get the ecg split into hear beats\n",
    "    hb,hb_labels,decoder=split_HB(numHB,ecg,labels,names,freq)\n",
    "    #find the optimal length if minL was not given\n",
    "    if minL<=0:\n",
    "        minL=int(find_optimal_length(hb))\n",
    "        \n",
    "     #transforms it into a list of np array\n",
    "    temp=list()\n",
    "    for idx,ecg_lead in enumerate(hb): \n",
    "        temp.append(ecg_lead.to_numpy())\n",
    "    #make all heart beat signals the same length, for larger signal cut them, for shorter signals just add 0s until it matches the desired length\n",
    "  \n",
    "    #new approach to delete the other signals that are too small\n",
    "    new_ecg=list()\n",
    "    new_labels=list()\n",
    "    #instead of removing them, we want to do something different, add them in another list\n",
    "    for idx, ecg_lead in enumerate(temp):\n",
    "        if len(ecg_lead)>=minL:\n",
    "            new_ecg.append(ecg_lead[:minL])\n",
    "            new_labels.append(hb_labels[idx])\n",
    "            #new_ecg_names.append(ecg_names[idx])\n",
    "        else:\n",
    "            toAdd=minL-len(ecg_lead)\n",
    "            toAdd=np.zeros(toAdd)\n",
    "            #merges the signal that was too short with signal full of 0 to achive the desired lenght\n",
    "            new_ecg.append(np.concatenate((ecg_lead, toAdd)))\n",
    "            new_labels.append(hb_labels[idx])\n",
    "            #new_ecg_names.append(ecg_names[idx])\n",
    "    if smote:\n",
    "        sm = SMOTE()\n",
    "        new_ecg, new_labels = sm.fit_resample(new_ecg, new_labels)\n",
    "\n",
    "    \n",
    "    #only use pca if we say so\n",
    "    if pca:\n",
    "        #if we dont send any older pca, then create its own\n",
    "        if pca_prev is None:\n",
    "            pca_prev=PCA(.99)\n",
    "            pca_prev.fit(new_ecg)\n",
    "        #use the pca to reduce the dimension of the data\n",
    "        new_ecg=pca_prev.transform(new_ecg)\n",
    "        data['pca']=pca_prev\n",
    "\n",
    " #now convert the data into working dataset for the cnn model\n",
    "    x=new_ecg\n",
    "    x=np.array(x)\n",
    "    x=x.reshape((x.shape[0],x.shape[1],1))\n",
    "    \n",
    "    y=np.array(new_labels)\n",
    "    \n",
    "    #return the data in a dictionary way, to simplily later use of the desired variables\n",
    "   \n",
    "    data['input']=x\n",
    "    data['labels']=y\n",
    "    data['decoder']=decoder\n",
    "    data['optimal_length']=minL\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31409ada",
   "metadata": {},
   "source": [
    "### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "126ec7f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n"
     ]
    }
   ],
   "source": [
    "train=generate_data_set(3,ecg_train,labels_train,names_train,fs,\n",
    "                        smote=True,\n",
    "                       )\n",
    "#train is a dictionary, get the actual variables\n",
    "x_train=train['input']\n",
    "y_train=train['labels']\n",
    "opt_length=train['optimal_length']\n",
    "print(\"----------\")\n",
    "test=generate_data_set(3,ecg_test,labels_test,names_test,fs,\n",
    "                       minL=opt_length\n",
    "                      )\n",
    "x_test=test['input']\n",
    "y_test=test['labels']\n",
    "decoder_test=test['decoder']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97106ec0",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05e121e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS THE MODEL WE ARE USING\n",
    "#CNN\n",
    "from tensorflow.keras.layers import Flatten, Dense, Conv1D, MaxPool1D, Dropout\n",
    "from keras.layers import GaussianNoise\n",
    "# Create sequential model \n",
    "cnn_model = tf.keras.models.Sequential()\n",
    "#First CNN layer  with 32 filters, conv window 3, relu activation and same padding\n",
    "cnn_model.add(Conv1D(filters=32, kernel_size=(5,), strides=1, padding='same', activation=tf.keras.layers.LeakyReLU(alpha=0.001), input_shape = (x_train.shape[1:])))\n",
    "cnn_model.add(Conv1D(filters=32, kernel_size=(5,), strides=1, padding='same', activation=tf.keras.layers.LeakyReLU(alpha=0.001), input_shape = (x_train.shape[1:])))\n",
    "\n",
    "#Second CNN layer  with 64 filters, conv window 3, relu activation and same padding\n",
    "cnn_model.add(Conv1D(filters=64, kernel_size=(3,), strides=1, padding='same', activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n",
    "#Third CNN layer with 128 filters, conv window 3, relu activation and same padding\n",
    "cnn_model.add(Conv1D(filters=64, kernel_size=(3,), strides=1,padding='same', activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n",
    "#Fourth CNN layer with Max pooling\n",
    "cnn_model.add(MaxPool1D(pool_size=(3,), strides=2, padding='same'))\n",
    "cnn_model.add(Dropout(0.5))\n",
    "\n",
    "#this is legit very important\n",
    "cnn_model.add(GaussianNoise(0.1))\n",
    "#Flatten the output\n",
    "\n",
    "cnn_model.add(Flatten())\n",
    "#Add a dense layer with 256 neurons\n",
    "cnn_model.add(Dense(units = 64, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n",
    "#Add a dense layer with 512 neurons\n",
    "cnn_model.add(Dense(units = 128, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n",
    "#Add a dense layer with 512 neurons\n",
    "cnn_model.add(Dense(units = 128, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n",
    "#Softmax as last layer with five outputs\n",
    "cnn_model.add(Dense(units = 4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a12c30da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OLD CNN MODEL\n",
    "#CNN\n",
    "from tensorflow.keras.layers import Flatten, Dense, Conv1D, MaxPool1D, Dropout\n",
    "from keras.layers import GaussianNoise\n",
    "# Create sequential model \n",
    "cnn_model = tf.keras.models.Sequential()\n",
    "#First CNN layer  with 32 filters, conv window 3, relu activation and same padding\n",
    "cnn_model.add(Conv1D(filters=32, kernel_size=(3,), strides=1, padding='same', activation=tf.keras.layers.LeakyReLU(alpha=0.001), input_shape = (x_train.shape[1:])))\n",
    "#Second CNN layer  with 64 filters, conv window 3, relu activation and same padding\n",
    "cnn_model.add(Conv1D(filters=64, kernel_size=(3,), strides=1, padding='same', activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n",
    "#Third CNN layer with 128 filters, conv window 3, relu activation and same padding\n",
    "cnn_model.add(Conv1D(filters=128, kernel_size=(3,), strides=1,padding='same', activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n",
    "#Fourth CNN layer with Max pooling\n",
    "cnn_model.add(MaxPool1D(pool_size=(3,), strides=2, padding='same'))\n",
    "cnn_model.add(Dropout(0.5))\n",
    "\n",
    "#this is legit very important\n",
    "cnn_model.add(GaussianNoise(0.1))\n",
    "#Flatten the output\n",
    "\n",
    "cnn_model.add(Flatten())\n",
    "#Add a dense layer with 256 neurons\n",
    "cnn_model.add(Dense(units = 64, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n",
    "#Add a dense layer with 512 neurons\n",
    "cnn_model.add(Dense(units = 128, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n",
    "#Add a dense layer with 512 neurons\n",
    "cnn_model.add(Dense(units = 256, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n",
    "#Softmax as last layer with five outputs\n",
    "cnn_model.add(Dense(units = 4, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dacecd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 715, 32)           192       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 715, 32)           5152      \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 715, 64)           6208      \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 715, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 358, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 358, 64)           0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise (GaussianNois (None, 358, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 22912)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                1466432   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 1,515,684\n",
      "Trainable params: 1,515,684\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_model.compile(optimizer='adam', loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02cb040",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b80528c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "4306/4306 [==============================] - 24s 5ms/step - loss: 1.8422 - accuracy: 0.5597 - val_loss: 1.2373 - val_accuracy: 0.5506\n",
      "Epoch 2/8\n",
      "4306/4306 [==============================] - 21s 5ms/step - loss: 1.2783 - accuracy: 0.6836 - val_loss: 1.1295 - val_accuracy: 0.5717\n",
      "Epoch 3/8\n",
      "4306/4306 [==============================] - 22s 5ms/step - loss: 1.0390 - accuracy: 0.7275 - val_loss: 1.1184 - val_accuracy: 0.6109\n",
      "Epoch 4/8\n",
      "4306/4306 [==============================] - 23s 5ms/step - loss: 0.9060 - accuracy: 0.7545 - val_loss: 1.0465 - val_accuracy: 0.6217\n",
      "Epoch 5/8\n",
      "4306/4306 [==============================] - 23s 5ms/step - loss: 0.8140 - accuracy: 0.7742 - val_loss: 1.0875 - val_accuracy: 0.6390\n",
      "Epoch 6/8\n",
      "4306/4306 [==============================] - 21s 5ms/step - loss: 0.7463 - accuracy: 0.7872 - val_loss: 1.0975 - val_accuracy: 0.6412\n",
      "Epoch 7/8\n",
      "4306/4306 [==============================] - 21s 5ms/step - loss: 0.6914 - accuracy: 0.8001 - val_loss: 1.0469 - val_accuracy: 0.6355\n",
      "Epoch 8/8\n",
      "4306/4306 [==============================] - 21s 5ms/step - loss: 0.6540 - accuracy: 0.8091 - val_loss: 1.1505 - val_accuracy: 0.6310\n"
     ]
    }
   ],
   "source": [
    "epochs = 8\n",
    "batch_size = 32\n",
    "weights = {0: 2.,1: 10.,2: 1.,3: 1}\n",
    "from keras import callbacks\n",
    "earlystopping = callbacks.EarlyStopping(monitor =\"val_loss\", mode =\"min\", patience = 5, restore_best_weights = True)\n",
    "history = cnn_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data = (x_test, y_test),\n",
    "class_weight=weights\n",
    "#callbacks =[earlystopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb9d290",
   "metadata": {},
   "source": [
    "### Get predictions and score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b516c9b5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200\t Labels wurden geschrieben.\n",
      "1200\t Labels wurden geschrieben.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8844221105527639,\n",
       " 0.5709409213460968,\n",
       " {'N': {'n': 654, 'a': 7, 'o': 36, 'p': 19},\n",
       "  'A': {'n': 10, 'a': 88, 'o': 2, 'p': 4},\n",
       "  'O': {'n': 180, 'a': 67, 'o': 89, 'p': 7},\n",
       "  'P': {'n': 12, 'a': 4, 'o': 3, 'p': 18}})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statistics\n",
    "from statistics import mode\n",
    "#get the predictions\n",
    "classes = cnn_model.predict(x_test)\n",
    "predictions=list()\n",
    "for idx, ecg_lead in enumerate (classes):\n",
    "    predictions.append(np.argmax(ecg_lead))\n",
    "\n",
    "#this dictionary will save all the predictions in groups\n",
    "label_decoder={}\n",
    "for idx, label in enumerate(predictions):\n",
    "    if decoder_test[idx] in label_decoder:\n",
    "        label_decoder[decoder_test[idx]].append(label)\n",
    "    else:\n",
    "        label_decoder[decoder_test[idx]]=list()\n",
    "        label_decoder[decoder_test[idx]].append(label)\n",
    "#to get the most frequent prediction for each signal \n",
    "labels_toTest=list()\n",
    "labels_toSave=list()\n",
    "for idx,keys in enumerate(label_decoder):\n",
    "    #get the most frequent prediction\n",
    "    frequent_label=mode(label_decoder[keys])\n",
    "    #convert it into a letter\n",
    "    labels2give=\"N\"\n",
    "    if int(frequent_label)==0:\n",
    "        labels2give=\"N\"\n",
    "    elif int(frequent_label)==1:\n",
    "        labels2give=\"A\"\n",
    "    elif int(frequent_label)==2:\n",
    "        labels2give=\"O\"\n",
    "    else:\n",
    "        labels2give=\"~\"\n",
    "    #save the final predictred labels\n",
    "    labels_toTest.append((keys,labels2give))\n",
    "\n",
    "for i,ecg in enumerate(labels_test):\n",
    "    ecg_class=int(ecg)\n",
    "    label2give=\"N\"\n",
    "    if int(ecg_class)==0:\n",
    "        labels2give=\"N\"\n",
    "    elif int(ecg_class)==1:\n",
    "        labels2give=\"A\"\n",
    "    elif int(ecg_class)==2:\n",
    "        labels2give=\"O\"\n",
    "    else:\n",
    "        labels2give=\"~\"\n",
    "    #esto es para el name\n",
    "    labels_toSave.append((names_test[i],labels2give))\n",
    "\n",
    "save_predictions(labels_toTest,model_dir,\"PREDICTIONS\")\n",
    "save_predictions(labels_toSave,model_dir,\"REFERENCE\")\n",
    "score(model_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
